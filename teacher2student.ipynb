{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"output/FastPitch_checkpoint_teacher_100.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['pitch_mean', 'pitch_std', 'encoder.word_emb.weight', 'encoder.pos_emb.inv_freq', 'encoder.layers.0.dec_attn.qkv_net.weight', 'encoder.layers.0.dec_attn.qkv_net.bias', 'encoder.layers.0.dec_attn.o_net.weight', 'encoder.layers.0.dec_attn.layer_norm.weight', 'encoder.layers.0.dec_attn.layer_norm.bias', 'encoder.layers.0.pos_ff.CoreNet.0.weight', 'encoder.layers.0.pos_ff.CoreNet.0.bias', 'encoder.layers.0.pos_ff.CoreNet.2.weight', 'encoder.layers.0.pos_ff.CoreNet.2.bias', 'encoder.layers.0.pos_ff.layer_norm.weight', 'encoder.layers.0.pos_ff.layer_norm.bias', 'encoder.layers.1.dec_attn.qkv_net.weight', 'encoder.layers.1.dec_attn.qkv_net.bias', 'encoder.layers.1.dec_attn.o_net.weight', 'encoder.layers.1.dec_attn.layer_norm.weight', 'encoder.layers.1.dec_attn.layer_norm.bias', 'encoder.layers.1.pos_ff.CoreNet.0.weight', 'encoder.layers.1.pos_ff.CoreNet.0.bias', 'encoder.layers.1.pos_ff.CoreNet.2.weight', 'encoder.layers.1.pos_ff.CoreNet.2.bias', 'encoder.layers.1.pos_ff.layer_norm.weight', 'encoder.layers.1.pos_ff.layer_norm.bias', 'encoder.layers.2.dec_attn.qkv_net.weight', 'encoder.layers.2.dec_attn.qkv_net.bias', 'encoder.layers.2.dec_attn.o_net.weight', 'encoder.layers.2.dec_attn.layer_norm.weight', 'encoder.layers.2.dec_attn.layer_norm.bias', 'encoder.layers.2.pos_ff.CoreNet.0.weight', 'encoder.layers.2.pos_ff.CoreNet.0.bias', 'encoder.layers.2.pos_ff.CoreNet.2.weight', 'encoder.layers.2.pos_ff.CoreNet.2.bias', 'encoder.layers.2.pos_ff.layer_norm.weight', 'encoder.layers.2.pos_ff.layer_norm.bias', 'encoder.layers.3.dec_attn.qkv_net.weight', 'encoder.layers.3.dec_attn.qkv_net.bias', 'encoder.layers.3.dec_attn.o_net.weight', 'encoder.layers.3.dec_attn.layer_norm.weight', 'encoder.layers.3.dec_attn.layer_norm.bias', 'encoder.layers.3.pos_ff.CoreNet.0.weight', 'encoder.layers.3.pos_ff.CoreNet.0.bias', 'encoder.layers.3.pos_ff.CoreNet.2.weight', 'encoder.layers.3.pos_ff.CoreNet.2.bias', 'encoder.layers.3.pos_ff.layer_norm.weight', 'encoder.layers.3.pos_ff.layer_norm.bias', 'encoder.layers.4.dec_attn.qkv_net.weight', 'encoder.layers.4.dec_attn.qkv_net.bias', 'encoder.layers.4.dec_attn.o_net.weight', 'encoder.layers.4.dec_attn.layer_norm.weight', 'encoder.layers.4.dec_attn.layer_norm.bias', 'encoder.layers.4.pos_ff.CoreNet.0.weight', 'encoder.layers.4.pos_ff.CoreNet.0.bias', 'encoder.layers.4.pos_ff.CoreNet.2.weight', 'encoder.layers.4.pos_ff.CoreNet.2.bias', 'encoder.layers.4.pos_ff.layer_norm.weight', 'encoder.layers.4.pos_ff.layer_norm.bias', 'encoder.layers.5.dec_attn.qkv_net.weight', 'encoder.layers.5.dec_attn.qkv_net.bias', 'encoder.layers.5.dec_attn.o_net.weight', 'encoder.layers.5.dec_attn.layer_norm.weight', 'encoder.layers.5.dec_attn.layer_norm.bias', 'encoder.layers.5.pos_ff.CoreNet.0.weight', 'encoder.layers.5.pos_ff.CoreNet.0.bias', 'encoder.layers.5.pos_ff.CoreNet.2.weight', 'encoder.layers.5.pos_ff.CoreNet.2.bias', 'encoder.layers.5.pos_ff.layer_norm.weight', 'encoder.layers.5.pos_ff.layer_norm.bias', 'duration_predictor.layers.0.conv.weight', 'duration_predictor.layers.0.conv.bias', 'duration_predictor.layers.0.norm.weight', 'duration_predictor.layers.0.norm.bias', 'duration_predictor.layers.1.conv.weight', 'duration_predictor.layers.1.conv.bias', 'duration_predictor.layers.1.norm.weight', 'duration_predictor.layers.1.norm.bias', 'duration_predictor.fc.weight', 'duration_predictor.fc.bias', 'decoder.pos_emb.inv_freq', 'decoder.layers.0.dec_attn.qkv_net.weight', 'decoder.layers.0.dec_attn.qkv_net.bias', 'decoder.layers.0.dec_attn.o_net.weight', 'decoder.layers.0.dec_attn.layer_norm.weight', 'decoder.layers.0.dec_attn.layer_norm.bias', 'decoder.layers.0.pos_ff.CoreNet.0.weight', 'decoder.layers.0.pos_ff.CoreNet.0.bias', 'decoder.layers.0.pos_ff.CoreNet.2.weight', 'decoder.layers.0.pos_ff.CoreNet.2.bias', 'decoder.layers.0.pos_ff.layer_norm.weight', 'decoder.layers.0.pos_ff.layer_norm.bias', 'decoder.layers.1.dec_attn.qkv_net.weight', 'decoder.layers.1.dec_attn.qkv_net.bias', 'decoder.layers.1.dec_attn.o_net.weight', 'decoder.layers.1.dec_attn.layer_norm.weight', 'decoder.layers.1.dec_attn.layer_norm.bias', 'decoder.layers.1.pos_ff.CoreNet.0.weight', 'decoder.layers.1.pos_ff.CoreNet.0.bias', 'decoder.layers.1.pos_ff.CoreNet.2.weight', 'decoder.layers.1.pos_ff.CoreNet.2.bias', 'decoder.layers.1.pos_ff.layer_norm.weight', 'decoder.layers.1.pos_ff.layer_norm.bias', 'decoder.layers.2.dec_attn.qkv_net.weight', 'decoder.layers.2.dec_attn.qkv_net.bias', 'decoder.layers.2.dec_attn.o_net.weight', 'decoder.layers.2.dec_attn.layer_norm.weight', 'decoder.layers.2.dec_attn.layer_norm.bias', 'decoder.layers.2.pos_ff.CoreNet.0.weight', 'decoder.layers.2.pos_ff.CoreNet.0.bias', 'decoder.layers.2.pos_ff.CoreNet.2.weight', 'decoder.layers.2.pos_ff.CoreNet.2.bias', 'decoder.layers.2.pos_ff.layer_norm.weight', 'decoder.layers.2.pos_ff.layer_norm.bias', 'decoder.layers.3.dec_attn.qkv_net.weight', 'decoder.layers.3.dec_attn.qkv_net.bias', 'decoder.layers.3.dec_attn.o_net.weight', 'decoder.layers.3.dec_attn.layer_norm.weight', 'decoder.layers.3.dec_attn.layer_norm.bias', 'decoder.layers.3.pos_ff.CoreNet.0.weight', 'decoder.layers.3.pos_ff.CoreNet.0.bias', 'decoder.layers.3.pos_ff.CoreNet.2.weight', 'decoder.layers.3.pos_ff.CoreNet.2.bias', 'decoder.layers.3.pos_ff.layer_norm.weight', 'decoder.layers.3.pos_ff.layer_norm.bias', 'decoder.layers.4.dec_attn.qkv_net.weight', 'decoder.layers.4.dec_attn.qkv_net.bias', 'decoder.layers.4.dec_attn.o_net.weight', 'decoder.layers.4.dec_attn.layer_norm.weight', 'decoder.layers.4.dec_attn.layer_norm.bias', 'decoder.layers.4.pos_ff.CoreNet.0.weight', 'decoder.layers.4.pos_ff.CoreNet.0.bias', 'decoder.layers.4.pos_ff.CoreNet.2.weight', 'decoder.layers.4.pos_ff.CoreNet.2.bias', 'decoder.layers.4.pos_ff.layer_norm.weight', 'decoder.layers.4.pos_ff.layer_norm.bias', 'decoder.layers.5.dec_attn.qkv_net.weight', 'decoder.layers.5.dec_attn.qkv_net.bias', 'decoder.layers.5.dec_attn.o_net.weight', 'decoder.layers.5.dec_attn.layer_norm.weight', 'decoder.layers.5.dec_attn.layer_norm.bias', 'decoder.layers.5.pos_ff.CoreNet.0.weight', 'decoder.layers.5.pos_ff.CoreNet.0.bias', 'decoder.layers.5.pos_ff.CoreNet.2.weight', 'decoder.layers.5.pos_ff.CoreNet.2.bias', 'decoder.layers.5.pos_ff.layer_norm.weight', 'decoder.layers.5.pos_ff.layer_norm.bias', 'pitch_predictor.layers.0.conv.weight', 'pitch_predictor.layers.0.conv.bias', 'pitch_predictor.layers.0.norm.weight', 'pitch_predictor.layers.0.norm.bias', 'pitch_predictor.layers.1.conv.weight', 'pitch_predictor.layers.1.conv.bias', 'pitch_predictor.layers.1.norm.weight', 'pitch_predictor.layers.1.norm.bias', 'pitch_predictor.fc.weight', 'pitch_predictor.fc.bias', 'pitch_emb.weight', 'pitch_emb.bias', 'energy_predictor.layers.0.conv.weight', 'energy_predictor.layers.0.conv.bias', 'energy_predictor.layers.0.norm.weight', 'energy_predictor.layers.0.norm.bias', 'energy_predictor.layers.1.conv.weight', 'energy_predictor.layers.1.conv.bias', 'energy_predictor.layers.1.norm.weight', 'energy_predictor.layers.1.norm.bias', 'energy_predictor.fc.weight', 'energy_predictor.fc.bias', 'energy_emb.weight', 'energy_emb.bias', 'proj.weight', 'proj.bias', 'attention.query_proj.0.conv.weight', 'attention.query_proj.0.conv.bias', 'attention.query_proj.2.conv.weight', 'attention.query_proj.2.conv.bias', 'attention.query_proj.4.conv.weight', 'attention.query_proj.4.conv.bias', 'attention.attn_proj.weight', 'attention.attn_proj.bias', 'attention.key_proj.0.conv.weight', 'attention.key_proj.0.conv.bias', 'attention.key_proj.2.conv.weight', 'attention.key_proj.2.conv.bias'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt[\"state_dict\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_layer_remove(layer_name):\n",
    "    if \"encoder.layers.1\" in layer_name:\n",
    "        return True\n",
    "    if \"encoder.layers.2\" in layer_name:\n",
    "        return True\n",
    "    if \"encoder.layers.4\" in layer_name:\n",
    "        return True\n",
    "    if \"encoder.layers.5\" in layer_name:\n",
    "        return True\n",
    "\n",
    "    if \"decoder.layers.1\" in layer_name:\n",
    "        return True\n",
    "    if \"decoder.layers.2\" in layer_name:\n",
    "        return True\n",
    "    if \"decoder.layers.4\" in layer_name:\n",
    "        return True\n",
    "    if \"decoder.layers.5\" in layer_name:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "    \n",
    "layer_to_remove = [i for i in ckpt[\"state_dict\"].keys() if check_layer_remove(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encoder.layers.1.dec_attn.qkv_net.weight',\n",
       " 'encoder.layers.1.dec_attn.qkv_net.bias',\n",
       " 'encoder.layers.1.dec_attn.o_net.weight',\n",
       " 'encoder.layers.1.dec_attn.layer_norm.weight',\n",
       " 'encoder.layers.1.dec_attn.layer_norm.bias',\n",
       " 'encoder.layers.1.pos_ff.CoreNet.0.weight',\n",
       " 'encoder.layers.1.pos_ff.CoreNet.0.bias',\n",
       " 'encoder.layers.1.pos_ff.CoreNet.2.weight',\n",
       " 'encoder.layers.1.pos_ff.CoreNet.2.bias',\n",
       " 'encoder.layers.1.pos_ff.layer_norm.weight',\n",
       " 'encoder.layers.1.pos_ff.layer_norm.bias',\n",
       " 'encoder.layers.2.dec_attn.qkv_net.weight',\n",
       " 'encoder.layers.2.dec_attn.qkv_net.bias',\n",
       " 'encoder.layers.2.dec_attn.o_net.weight',\n",
       " 'encoder.layers.2.dec_attn.layer_norm.weight',\n",
       " 'encoder.layers.2.dec_attn.layer_norm.bias',\n",
       " 'encoder.layers.2.pos_ff.CoreNet.0.weight',\n",
       " 'encoder.layers.2.pos_ff.CoreNet.0.bias',\n",
       " 'encoder.layers.2.pos_ff.CoreNet.2.weight',\n",
       " 'encoder.layers.2.pos_ff.CoreNet.2.bias',\n",
       " 'encoder.layers.2.pos_ff.layer_norm.weight',\n",
       " 'encoder.layers.2.pos_ff.layer_norm.bias',\n",
       " 'encoder.layers.4.dec_attn.qkv_net.weight',\n",
       " 'encoder.layers.4.dec_attn.qkv_net.bias',\n",
       " 'encoder.layers.4.dec_attn.o_net.weight',\n",
       " 'encoder.layers.4.dec_attn.layer_norm.weight',\n",
       " 'encoder.layers.4.dec_attn.layer_norm.bias',\n",
       " 'encoder.layers.4.pos_ff.CoreNet.0.weight',\n",
       " 'encoder.layers.4.pos_ff.CoreNet.0.bias',\n",
       " 'encoder.layers.4.pos_ff.CoreNet.2.weight',\n",
       " 'encoder.layers.4.pos_ff.CoreNet.2.bias',\n",
       " 'encoder.layers.4.pos_ff.layer_norm.weight',\n",
       " 'encoder.layers.4.pos_ff.layer_norm.bias',\n",
       " 'encoder.layers.5.dec_attn.qkv_net.weight',\n",
       " 'encoder.layers.5.dec_attn.qkv_net.bias',\n",
       " 'encoder.layers.5.dec_attn.o_net.weight',\n",
       " 'encoder.layers.5.dec_attn.layer_norm.weight',\n",
       " 'encoder.layers.5.dec_attn.layer_norm.bias',\n",
       " 'encoder.layers.5.pos_ff.CoreNet.0.weight',\n",
       " 'encoder.layers.5.pos_ff.CoreNet.0.bias',\n",
       " 'encoder.layers.5.pos_ff.CoreNet.2.weight',\n",
       " 'encoder.layers.5.pos_ff.CoreNet.2.bias',\n",
       " 'encoder.layers.5.pos_ff.layer_norm.weight',\n",
       " 'encoder.layers.5.pos_ff.layer_norm.bias',\n",
       " 'decoder.layers.1.dec_attn.qkv_net.weight',\n",
       " 'decoder.layers.1.dec_attn.qkv_net.bias',\n",
       " 'decoder.layers.1.dec_attn.o_net.weight',\n",
       " 'decoder.layers.1.dec_attn.layer_norm.weight',\n",
       " 'decoder.layers.1.dec_attn.layer_norm.bias',\n",
       " 'decoder.layers.1.pos_ff.CoreNet.0.weight',\n",
       " 'decoder.layers.1.pos_ff.CoreNet.0.bias',\n",
       " 'decoder.layers.1.pos_ff.CoreNet.2.weight',\n",
       " 'decoder.layers.1.pos_ff.CoreNet.2.bias',\n",
       " 'decoder.layers.1.pos_ff.layer_norm.weight',\n",
       " 'decoder.layers.1.pos_ff.layer_norm.bias',\n",
       " 'decoder.layers.2.dec_attn.qkv_net.weight',\n",
       " 'decoder.layers.2.dec_attn.qkv_net.bias',\n",
       " 'decoder.layers.2.dec_attn.o_net.weight',\n",
       " 'decoder.layers.2.dec_attn.layer_norm.weight',\n",
       " 'decoder.layers.2.dec_attn.layer_norm.bias',\n",
       " 'decoder.layers.2.pos_ff.CoreNet.0.weight',\n",
       " 'decoder.layers.2.pos_ff.CoreNet.0.bias',\n",
       " 'decoder.layers.2.pos_ff.CoreNet.2.weight',\n",
       " 'decoder.layers.2.pos_ff.CoreNet.2.bias',\n",
       " 'decoder.layers.2.pos_ff.layer_norm.weight',\n",
       " 'decoder.layers.2.pos_ff.layer_norm.bias',\n",
       " 'decoder.layers.4.dec_attn.qkv_net.weight',\n",
       " 'decoder.layers.4.dec_attn.qkv_net.bias',\n",
       " 'decoder.layers.4.dec_attn.o_net.weight',\n",
       " 'decoder.layers.4.dec_attn.layer_norm.weight',\n",
       " 'decoder.layers.4.dec_attn.layer_norm.bias',\n",
       " 'decoder.layers.4.pos_ff.CoreNet.0.weight',\n",
       " 'decoder.layers.4.pos_ff.CoreNet.0.bias',\n",
       " 'decoder.layers.4.pos_ff.CoreNet.2.weight',\n",
       " 'decoder.layers.4.pos_ff.CoreNet.2.bias',\n",
       " 'decoder.layers.4.pos_ff.layer_norm.weight',\n",
       " 'decoder.layers.4.pos_ff.layer_norm.bias',\n",
       " 'decoder.layers.5.dec_attn.qkv_net.weight',\n",
       " 'decoder.layers.5.dec_attn.qkv_net.bias',\n",
       " 'decoder.layers.5.dec_attn.o_net.weight',\n",
       " 'decoder.layers.5.dec_attn.layer_norm.weight',\n",
       " 'decoder.layers.5.dec_attn.layer_norm.bias',\n",
       " 'decoder.layers.5.pos_ff.CoreNet.0.weight',\n",
       " 'decoder.layers.5.pos_ff.CoreNet.0.bias',\n",
       " 'decoder.layers.5.pos_ff.CoreNet.2.weight',\n",
       " 'decoder.layers.5.pos_ff.CoreNet.2.bias',\n",
       " 'decoder.layers.5.pos_ff.layer_norm.weight',\n",
       " 'decoder.layers.5.pos_ff.layer_norm.bias']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open(\"fastpitch_student.json\")\n",
    "student_config = json.load(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in layer_to_remove:\n",
    "    del ckpt[\"state_dict\"][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0283, -0.0030, -0.0215,  ..., -0.0244,  0.0299, -0.0393],\n",
       "        [ 0.0071, -0.0422,  0.0114,  ..., -0.0229, -0.0753, -0.0458],\n",
       "        [-0.0087, -0.0292, -0.0314,  ...,  0.0118,  0.0223,  0.0175],\n",
       "        ...,\n",
       "        [ 0.0154, -0.0119, -0.0037,  ...,  0.0309,  0.0335, -0.0146],\n",
       "        [ 0.0124, -0.0238,  0.0190,  ...,  0.0360, -0.0516, -0.0291],\n",
       "        [ 0.0349,  0.0014, -0.0078,  ..., -0.0241, -0.0313, -0.0046]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt[\"state_dict\"][\"encoder.layers.3.dec_attn.qkv_net.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in ckpt[\"state_dict\"].keys():\n",
    "    if \"encoder.layers.3\" in layer:\n",
    "        print(layer)\n",
    "        new_layer = layer.replace(\"encoder.layers.3\", \"encoder.layers.1\")\n",
    "        print(new_layer)\n",
    "        ckpt[\"state_dict\"][new_layer] = ckpt[\"state_dict\"].pop(layer)\n",
    "    if \"decoder.layers.3\" in layer:\n",
    "        print(layer)\n",
    "        new_layer = layer.replace(\"decoder.layers.3\", \"decoder.layers.1\")\n",
    "        print(new_layer)\n",
    "        ckpt[\"state_dict\"][new_layer] = ckpt[\"state_dict\"].pop(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ckpt[\"state_dict\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "\n",
    "student_model = models.get_model('FastPitch', student_config, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FastPitch(\n",
       "  (encoder): FFTransformer(\n",
       "    (word_emb): Embedding(148, 384, padding_idx=0)\n",
       "    (pos_emb): PositionalEmbedding()\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerLayer(\n",
       "        (dec_attn): MultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (dropatt): Dropout(p=0.1, inplace=False)\n",
       "          (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "          (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_ff): PositionwiseConvFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (duration_predictor): TemporalPredictor(\n",
       "    (layers): Sequential(\n",
       "      (0): ConvReLUNorm(\n",
       "        (conv): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): ConvReLUNorm(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (decoder): FFTransformer(\n",
       "    (pos_emb): PositionalEmbedding()\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerLayer(\n",
       "        (dec_attn): MultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (dropatt): Dropout(p=0.1, inplace=False)\n",
       "          (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "          (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_ff): PositionwiseConvFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pitch_predictor): TemporalPredictor(\n",
       "    (layers): Sequential(\n",
       "      (0): ConvReLUNorm(\n",
       "        (conv): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): ConvReLUNorm(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (pitch_emb): Conv1d(1, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (energy_predictor): TemporalPredictor(\n",
       "    (layers): Sequential(\n",
       "      (0): ConvReLUNorm(\n",
       "        (conv): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): ConvReLUNorm(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (energy_emb): Conv1d(1, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (proj): Linear(in_features=384, out_features=80, bias=True)\n",
       "  (attention): ConvAttention(\n",
       "    (softmax): Softmax(dim=3)\n",
       "    (log_softmax): LogSoftmax(dim=3)\n",
       "    (query_proj): Sequential(\n",
       "      (0): ConvNorm(\n",
       "        (conv): Conv1d(80, 160, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): ConvNorm(\n",
       "        (conv): Conv1d(160, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (3): ReLU()\n",
       "      (4): ConvNorm(\n",
       "        (conv): Conv1d(80, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (attn_proj): Conv2d(80, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (key_proj): Sequential(\n",
       "      (0): ConvNorm(\n",
       "        (conv): Conv1d(384, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): ConvNorm(\n",
       "        (conv): Conv1d(768, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.load_state_dict(ckpt[\"state_dict\"])\n",
    "student_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(student_model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"state_dict\": student_model.state_dict()}, \"student_100.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FastPitch(\n",
       "  (encoder): FFTransformer(\n",
       "    (word_emb): Embedding(148, 384, padding_idx=0)\n",
       "    (pos_emb): PositionalEmbedding()\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerLayer(\n",
       "        (dec_attn): MultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (dropatt): Dropout(p=0.1, inplace=False)\n",
       "          (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "          (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_ff): PositionwiseConvFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (duration_predictor): TemporalPredictor(\n",
       "    (layers): Sequential(\n",
       "      (0): ConvReLUNorm(\n",
       "        (conv): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): ConvReLUNorm(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (decoder): FFTransformer(\n",
       "    (pos_emb): PositionalEmbedding()\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerLayer(\n",
       "        (dec_attn): MultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (dropatt): Dropout(p=0.1, inplace=False)\n",
       "          (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "          (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_ff): PositionwiseConvFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pitch_predictor): TemporalPredictor(\n",
       "    (layers): Sequential(\n",
       "      (0): ConvReLUNorm(\n",
       "        (conv): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): ConvReLUNorm(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (pitch_emb): Conv1d(1, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (energy_predictor): TemporalPredictor(\n",
       "    (layers): Sequential(\n",
       "      (0): ConvReLUNorm(\n",
       "        (conv): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): ConvReLUNorm(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (energy_emb): Conv1d(1, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (proj): Linear(in_features=384, out_features=80, bias=True)\n",
       "  (attention): ConvAttention(\n",
       "    (softmax): Softmax(dim=3)\n",
       "    (log_softmax): LogSoftmax(dim=3)\n",
       "    (query_proj): Sequential(\n",
       "      (0): ConvNorm(\n",
       "        (conv): Conv1d(80, 160, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): ConvNorm(\n",
       "        (conv): Conv1d(160, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (3): ReLU()\n",
       "      (4): ConvNorm(\n",
       "        (conv): Conv1d(80, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (attn_proj): Conv2d(80, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (key_proj): Sequential(\n",
       "      (0): ConvNorm(\n",
       "        (conv): Conv1d(384, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): ConvNorm(\n",
       "        (conv): Conv1d(768, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_ckpt = torch.load(\"test_student.pt\")\n",
    "student_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'student_ckpt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m unwrap \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m m: \u001b[39mgetattr\u001b[39m(m, \u001b[39m\"\u001b[39m\u001b[39mmodule\u001b[39m\u001b[39m\"\u001b[39m, m)\n\u001b[0;32m----> 3\u001b[0m torch\u001b[39m.\u001b[39msave({\u001b[39m\"\u001b[39m\u001b[39mstate_dict\u001b[39m\u001b[39m\"\u001b[39m: unwrap(student_ckpt)\u001b[39m.\u001b[39mstate_dict()}, \u001b[39m\"\u001b[39m\u001b[39mtest_student_state_dict.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'student_ckpt' is not defined"
     ]
    }
   ],
   "source": [
    "unwrap = lambda m: getattr(m, \"module\", m)\n",
    "\n",
    "torch.save({\"state_dict\": unwrap(student_ckpt).state_dict()}, \"test_student_state_dict.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FastPitch(\n",
       "  (encoder): FFTransformer(\n",
       "    (word_emb): Embedding(148, 384, padding_idx=0)\n",
       "    (pos_emb): PositionalEmbedding()\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerLayer(\n",
       "        (dec_attn): MultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (dropatt): Dropout(p=0.1, inplace=False)\n",
       "          (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "          (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_ff): PositionwiseConvFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (duration_predictor): TemporalPredictor(\n",
       "    (layers): Sequential(\n",
       "      (0): ConvReLUNorm(\n",
       "        (conv): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): ConvReLUNorm(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (decoder): FFTransformer(\n",
       "    (pos_emb): PositionalEmbedding()\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerLayer(\n",
       "        (dec_attn): MultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=384, out_features=192, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (dropatt): Dropout(p=0.1, inplace=False)\n",
       "          (o_net): Linear(in_features=64, out_features=384, bias=False)\n",
       "          (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_ff): PositionwiseConvFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Conv1d(384, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): ReLU()\n",
       "            (2): Conv1d(1536, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pitch_predictor): TemporalPredictor(\n",
       "    (layers): Sequential(\n",
       "      (0): ConvReLUNorm(\n",
       "        (conv): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): ConvReLUNorm(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (pitch_emb): Conv1d(1, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (energy_predictor): TemporalPredictor(\n",
       "    (layers): Sequential(\n",
       "      (0): ConvReLUNorm(\n",
       "        (conv): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): ConvReLUNorm(\n",
       "        (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (energy_emb): Conv1d(1, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (proj): Linear(in_features=384, out_features=80, bias=True)\n",
       "  (attention): ConvAttention(\n",
       "    (softmax): Softmax(dim=3)\n",
       "    (log_softmax): LogSoftmax(dim=3)\n",
       "    (query_proj): Sequential(\n",
       "      (0): ConvNorm(\n",
       "        (conv): Conv1d(80, 160, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): ConvNorm(\n",
       "        (conv): Conv1d(160, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (3): ReLU()\n",
       "      (4): ConvNorm(\n",
       "        (conv): Conv1d(80, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (attn_proj): Conv2d(80, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (key_proj): Sequential(\n",
       "      (0): ConvNorm(\n",
       "        (conv): Conv1d(384, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): ConvNorm(\n",
       "        (conv): Conv1d(768, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "student_model = models.get_model('FastPitch', student_config, device=\"cpu\")\n",
    "\n",
    "ckpt = torch.load(\"student_100.pt\")\n",
    "print(1)\n",
    "\n",
    "no_pref = lambda sd: {re.sub(\"^module.\", \"\", k): v for k, v in sd.items()}\n",
    "unwrap = lambda m: getattr(m, \"module\", m)\n",
    "\n",
    "unwrap(student_model).load_state_dict(no_pref(ckpt[\"state_dict\"]))\n",
    "# student_model.load_state_dict(ckpt[\"state_dict\"])\n",
    "student_model.eval()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da2f6209576079fceb0349388ee27bf2a69e7b48d561b5338d33f1571a0f5ea3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 ('whisper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
